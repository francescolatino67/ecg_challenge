{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4y1ApsS9Xicf"
   },
   "source": [
    "# POLITECNICO DI MILANO\n",
    "# PhD Course AI CHALLENGES IN BIOENGINEERING 2025-26\n",
    "## ECG Challenge: CNN example\n",
    "---\n",
    "### Pietro Cerveri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6410,
     "status": "ok",
     "timestamp": 1770388694370,
     "user": {
      "displayName": "pietro cerveri",
      "userId": "02560383099036907904"
     },
     "user_tz": -60
    },
    "id": "vnDyNHJBzWcC"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "# from google.colab import drive\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score, confusion_matrix, balanced_accuracy_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1197,
     "status": "ok",
     "timestamp": 1770388698143,
     "user": {
      "displayName": "pietro cerveri",
      "userId": "02560383099036907904"
     },
     "user_tz": -60
    },
    "id": "jBbd7OfoX0qi",
    "outputId": "c4480907-aea8-4f21-daee-48e00847795a"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# # Set your own specific drive path\n",
    "ROOT_PATH   = os.path.dirname(os.getcwd())\n",
    "DATA_FOLDER   = os.path.join(ROOT_PATH, \"Data\")\n",
    "DATA_Batch_01 = os.path.join(DATA_FOLDER, \"01_batch_ECG_Signals\")\n",
    "DATA_Batch_02 = os.path.join(DATA_FOLDER, \"02_batch_ECG_Signals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1770388701557,
     "user": {
      "displayName": "pietro cerveri",
      "userId": "02560383099036907904"
     },
     "user_tz": -60
    },
    "id": "_JvYZ2MW6q6Z"
   },
   "outputs": [],
   "source": [
    "#prepare functions for filtering\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def apply_bandpass_filter(data, lowcut=1, highcut=40, fs=500, order=2):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def notch_filter(data, freq=50, fs=500, quality_factor=30):\n",
    "    b, a = iirnotch(freq / (fs / 2), quality_factor)\n",
    "    return filtfilt(b, a, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 412519,
     "status": "ok",
     "timestamp": 1770389119464,
     "user": {
      "displayName": "pietro cerveri",
      "userId": "02560383099036907904"
     },
     "user_tz": -60
    },
    "id": "Fg8yAEXI0Mqc",
    "outputId": "d7c12709-431e-4457-adf4-f740125fb2c2"
   },
   "outputs": [],
   "source": [
    "#import the data and filter the signals\n",
    "# This could be change depending if you download or not the data\n",
    "\n",
    "filename_Batch_01     = DATA_FOLDER + \"VALETUDO_database_1st_batch_en_all_info.xlsx\"\n",
    "filename_Batch_02     = DATA_FOLDER + \"VALETUDO_database_2nd_batch_en_all_info.xlsx\"\n",
    "tabular_data_Batch_01 = pd.read_excel(filename_Batch_01)\n",
    "tabular_data_Batch_02 = pd.read_excel(filename_Batch_02)\n",
    "\n",
    "# --- Load and filter both batches ---\n",
    "ECGs_1 = [f for f in os.listdir(DATA_Batch_01) if f.endswith(\".mat\")]\n",
    "ECGs_2 = [f for f in os.listdir(DATA_Batch_02) if f.endswith(\".mat\")]\n",
    "\n",
    "def extract_patient_id(filename):\n",
    "    return int(filename.split(\".\")[0])\n",
    "\n",
    "ECGs_1.sort(key=extract_patient_id)\n",
    "ECGs_2.sort(key=extract_patient_id)\n",
    "\n",
    "signals_1 = np.empty((len(ECGs_1), 5000, 12))\n",
    "signals_2 = np.empty((len(ECGs_2), 5000, 12))\n",
    "\n",
    "for index, ecg_path in enumerate(ECGs_1):\n",
    "    filepath = os.path.join(DATA_Batch_01, ecg_path)\n",
    "    matdata = scipy.io.loadmat(filepath)\n",
    "    ecg = matdata['val']\n",
    "    for i in range(12):\n",
    "        ecg[:, i] = ecg[:, i] - np.mean(ecg[:, i])\n",
    "        ecg[:, i] = apply_bandpass_filter(ecg[:, i])\n",
    "        ecg[:, i] = notch_filter(ecg[:, i])\n",
    "    signals_1[index, :, :] = ecg\n",
    "\n",
    "for index, ecg_path in enumerate(ECGs_2):\n",
    "    filepath = os.path.join(DATA_Batch_02, ecg_path)\n",
    "    matdata = scipy.io.loadmat(filepath)\n",
    "    ecg = matdata['val']\n",
    "    for i in range(12):\n",
    "        ecg[:, i] = ecg[:, i] - np.mean(ecg[:, i])\n",
    "        ecg[:, i] = apply_bandpass_filter(ecg[:, i])\n",
    "        ecg[:, i] = notch_filter(ecg[:, i])\n",
    "    signals_2[index, :, :] = ecg\n",
    "\n",
    "# --- Concatenate signals and tabular data ---\n",
    "signals = np.concatenate([signals_1, signals_2], axis=0)\n",
    "tabular_data = pd.concat([\n",
    "    tabular_data_Batch_01.sort_values(by=\"ECG_patient_id\").reset_index(drop=True),\n",
    "    tabular_data_Batch_02.sort_values(by=\"ECG_patient_id\").reset_index(drop=True)\n",
    "], ignore_index=True)\n",
    "\n",
    "print(\"Combined signals shape:\", signals.shape)\n",
    "print(\"Combined tabular shape:\", tabular_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1770389550942,
     "user": {
      "displayName": "pietro cerveri",
      "userId": "02560383099036907904"
     },
     "user_tz": -60
    },
    "id": "qgWV54O3a2YC",
    "outputId": "ea4cacc4-f85b-4d28-b393-cbadf82fc374"
   },
   "outputs": [],
   "source": [
    "print(f\"nb pos: {np.sum(tabular_data['sport_ability']==1)}\")\n",
    "print(f\"% pos: {np.sum(tabular_data['sport_ability']==1)/len(tabular_data['sport_ability'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1770389553689,
     "user": {
      "displayName": "pietro cerveri",
      "userId": "02560383099036907904"
     },
     "user_tz": -60
    },
    "id": "OFUQ1LVd8aMT"
   },
   "outputs": [],
   "source": [
    "#function to extract segment\n",
    "\n",
    "def segment_ecg(signal, tabular_data, segment_length=2500):\n",
    "\n",
    "    segments = np.zeros((signal.shape[0], segment_length, signal.shape[2]))\n",
    "    index = 0\n",
    "    for i in range(signal.shape[0]):\n",
    "\n",
    "      start = 0 # we take the start at the middle or elsewhere\n",
    "      end = start + segment_length\n",
    "      segments[index, :, :] = signal[i, start:end, :]\n",
    "      index += 1\n",
    "\n",
    "    return segments, tabular_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1770389558360,
     "user": {
      "displayName": "pietro cerveri",
      "userId": "02560383099036907904"
     },
     "user_tz": -60
    },
    "id": "eBiRFcSkljcC"
   },
   "outputs": [],
   "source": [
    "#prepare the class for the dataset without data augmentation\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, tabular, signals, labels):\n",
    "        self.signals = torch.tensor(signals, dtype=torch.float32).permute(0, 2, 1)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.float32).unsqueeze(1)\n",
    "        self.tabular = torch.tensor(tabular.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tabular[idx], self.signals[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1770389606114,
     "user": {
      "displayName": "pietro cerveri",
      "userId": "02560383099036907904"
     },
     "user_tz": -60
    },
    "id": "Wr7zQ7aICUvI"
   },
   "outputs": [],
   "source": [
    "# architecture of the three branches model\n",
    "# In this CNN we have 3 branches one for the 6 first leads (crop in segments of 2500), one for the 6 other lead in the same format and one branch for the tabular data (6 features)\n",
    "\n",
    "\n",
    "class ThreeBranchGRUModel(nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_layers=1):\n",
    "        super(ThreeBranchGRUModel, self).__init__()\n",
    "\n",
    "        self.fc1_tabular = nn.Linear(6, 16)\n",
    "        self.fc2_tabular = nn.Linear(16, 32)\n",
    "\n",
    "        self.conv1_1 = nn.Conv1d(in_channels=6, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2_1 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv1_2 = nn.Conv1d(in_channels=6, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2_2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.gru_signal = nn.GRU(input_size=64, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        self.fc1_final = nn.Linear(hidden_size * 2 + 32, 128)\n",
    "        self.fc2_final = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, tab, sig):\n",
    "        #print(f\"tab input shape: {tab.shape}, sig input shape: {sig.shape}\")\n",
    "        x_tabular = F.gelu(self.fc1_tabular(tab))\n",
    "        #print(f\"x_tabular shape after the first layer: {x_tabular.shape}\")\n",
    "        x_tabular = self.fc2_tabular(x_tabular)\n",
    "        #print(f\"x_tabular shape after the first layer: {x_tabular.shape}\")\n",
    "\n",
    "        sig1 = sig[:, :6, :]\n",
    "        #print(f\"sig1 input shape: {sig1.shape}\")\n",
    "        sig1 = self.pool(F.gelu(self.conv1_1(sig1)))\n",
    "        #print(f\"sig1 shape after the first layer: {sig1.shape}\")\n",
    "        sig1 = self.pool(F.gelu(self.conv2_1(sig1)))\n",
    "        #print(f\"sig1 shape after the second layer: {sig1.shape}\")\n",
    "        sig1 = sig1.permute(0, 2, 1)\n",
    "        _, x_sig1 = self.gru_signal(sig1)\n",
    "        #print(f\"x_sig1 shape: {x_sig1.shape}\")\n",
    "\n",
    "        sig2 = sig[:, 6:, :]\n",
    "        #print(f\"sig2 input shape: {sig2.shape}\")\n",
    "        sig2 = self.pool(F.gelu(self.conv1_2(sig2)))\n",
    "        #print(f\"sig2 shape after the first layer: {sig2.shape}\")\n",
    "        sig2 = self.pool(F.gelu(self.conv2_2(sig2)))\n",
    "        #print(f\"sig2 shape after the second layer: {sig2.shape}\")\n",
    "        sig2 = sig2.permute(0, 2, 1)\n",
    "        _, x_sig2 = self.gru_signal(sig2)\n",
    "        #print(f\"x_sig2 shape: {x_sig2.shape}\")\n",
    "\n",
    "        x = torch.cat((x_sig1[-1], x_sig2[-1]), dim=1)\n",
    "        #print(f\"x shape after branches concatenation: {x.shape}\")\n",
    "        x = torch.cat((x, x_tabular), dim=1)\n",
    "        #print(f\"x shape after tabular concatenation: {x.shape}\")\n",
    "        x = F.gelu(self.fc1_final(x))\n",
    "        #print(f\"x shape after final processing: {x.shape}\")\n",
    "        x = torch.sigmoid(self.fc2_final(x))\n",
    "        #print(f\"output shape: {x.shape}\")\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18218,
     "status": "ok",
     "timestamp": 1770389630453,
     "user": {
      "displayName": "pietro cerveri",
      "userId": "02560383099036907904"
     },
     "user_tz": -60
    },
    "id": "jApmRWLy3gvM",
    "outputId": "2592140a-c6a3-4c59-b053-224915d8a844"
   },
   "outputs": [],
   "source": [
    "#implementation of the ten fold cross-validation\n",
    "\n",
    "strat_kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "f1_list_all_folds = []\n",
    "f1_list_all_folds_train = []\n",
    "sensitivity_list_all_folds = []\n",
    "sensitivity_list_all_folds_train = []\n",
    "specificity_list_all_folds = []\n",
    "specificity_list_all_folds_train = []\n",
    "accuracy_list_all_folds = []\n",
    "accuracy_list_all_folds_train = []\n",
    "auc_score_list_all_folds = []\n",
    "auc_score_list_all_folds_train = []\n",
    "fpr_list_all_folds = []\n",
    "tpr_list_all_folds = []\n",
    "test_loss_all_folds = []\n",
    "train_loss_all_folds = []\n",
    "epochs_all_fold = []\n",
    "train_loss_max = []\n",
    "test_loss_max = []\n",
    "\n",
    "num_epocs = 5 #num_epocs = 50\n",
    "\n",
    "for train_index, test_index in strat_kf.split(tabular_data, tabular_data['sport_ability']):\n",
    "    X_train, X_test = tabular_data.iloc[train_index,:], tabular_data.iloc[test_index,:]\n",
    "    ecg_train = signals[train_index,:,:]\n",
    "    ecg_test = signals[test_index,:,:]\n",
    "\n",
    "    ecg_train_segments, tabular_train_expand = segment_ecg(ecg_train, X_train)\n",
    "    ecg_test_segments, tabular_test_expand = segment_ecg(ecg_test, X_test)\n",
    "\n",
    "    Y_train = tabular_train_expand['sport_ability']\n",
    "    Y_test = tabular_test_expand['sport_ability']\n",
    "\n",
    "    X_train_final = tabular_train_expand.drop(columns=['sport_ability', 'ECG_patient_id'])\n",
    "    X_test_final = tabular_test_expand.drop(columns=['sport_ability', 'ECG_patient_id'])\n",
    "\n",
    "    X_train_final['age_at_exam'] = X_train_final['age_at_exam'].apply(lambda x: x if 0.0 <= x <= 100.0 else np.nan)\n",
    "    X_train_final['trainning_load'] = X_train_final['trainning_load'].apply(lambda x: x if 0 < x <= 4 else np.nan)\n",
    "\n",
    "    imputer = IterativeImputer()\n",
    "    X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_final), columns=X_train_final.columns)\n",
    "\n",
    "    categorical_cols = ['sex', 'sport_classification']\n",
    "    numeric_cols = ['age_at_exam', 'height', 'weight', 'trainning_load']\n",
    "\n",
    "    X_test_final['age_at_exam'] = X_test_final['age_at_exam'].apply(lambda x: x if 0.0 <= x <= 100.0 else np.nan)\n",
    "    X_test_final['trainning_load'] = X_test_final['trainning_load'].apply(lambda x: x if 0 < x <= 4 else np.nan)\n",
    "\n",
    "    X_test_imputed = pd.DataFrame(imputer.transform(X_test_final), columns=X_test_final.columns)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_imputed[numeric_cols] = scaler.fit_transform(X_train_imputed[numeric_cols])\n",
    "    X_test_imputed[numeric_cols] = scaler.transform(X_test_imputed[numeric_cols])\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        X_train_imputed[col] = X_train_imputed[col].apply(lambda x: -1 if x == 0 else x)\n",
    "        X_test_imputed[col] = X_test_imputed[col].apply(lambda x: -1 if x == 0 else x)\n",
    "\n",
    "    train_final_df = pd.concat([X_train_imputed[numeric_cols], X_train_imputed[categorical_cols]], axis=1)\n",
    "    test_final_df = pd.concat([X_test_imputed[numeric_cols], X_test_imputed[categorical_cols]], axis=1)\n",
    "\n",
    "    train_dataset = ECGDataset(train_final_df, ecg_train_segments, Y_train)\n",
    "    test_dataset = ECGDataset(test_final_df, ecg_test_segments, Y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = ThreeBranchGRUModel().to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    f1_list_single_fold = []\n",
    "    f1_list_single_fold_train = []\n",
    "    sensitivity_list_single_fold = []\n",
    "    sensitivity_list_single_fold_train = []\n",
    "    specificity_list_single_fold = []\n",
    "    specificity_list_single_fold_train = []\n",
    "    accuracy_list_single_fold = []\n",
    "    accuracy_list_single_fold_train = []\n",
    "    auc_score_list_single_fold = []\n",
    "    auc_score_list_single_fold_train = []\n",
    "    fpr_list_single_fold = []\n",
    "    tpr_list_single_fold = []\n",
    "    train_loss_single_fold = []\n",
    "    test_loss_single_fold = []\n",
    "    epochs_single_fold = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epocs)):\n",
    "\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_outputs = []\n",
    "\n",
    "        for tabular, signals_ecg, labels in train_loader:\n",
    "            model.train()\n",
    "            tabular, signals_ecg, labels = tabular.to(device), signals_ecg.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(tabular, signals_ecg)\n",
    "            loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            predicted = (outputs > 0.6).int()\n",
    "            labels = labels.int()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu())\n",
    "            all_preds.extend(predicted.cpu())\n",
    "            all_outputs.extend(outputs.cpu())\n",
    "\n",
    "        print('\\n')\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader)}\")\n",
    "\n",
    "        train_loss_single_fold.append(train_loss/len(train_loader))\n",
    "        train_accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        auc_score = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "        f1_list_single_fold_train.append(f1)\n",
    "        sensitivity_list_single_fold_train.append(sensitivity)\n",
    "        specificity_list_single_fold_train.append(specificity)\n",
    "        accuracy_list_single_fold_train.append(train_accuracy)\n",
    "        auc_score_list_single_fold_train.append(auc_score)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_outputs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            correct = 0\n",
    "            test_loss = 0\n",
    "\n",
    "            for tabular, signals_ecg, labels in test_loader:\n",
    "\n",
    "                tabular, signals_ecg, labels = tabular.to(device), signals_ecg.to(device), labels.to(device)\n",
    "                outputs = model(tabular, signals_ecg)\n",
    "                loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "                test_loss += loss.item()\n",
    "                predicted = (outputs.squeeze() > 0.6).int()\n",
    "                labels = labels.squeeze().int()\n",
    "                correct += (predicted.squeeze() == labels).sum().item()\n",
    "                all_labels.extend(labels.cpu())\n",
    "                all_preds.extend(predicted.cpu())\n",
    "                all_outputs.extend(outputs.squeeze().cpu())\n",
    "\n",
    "            test_loss /= len(test_loader)\n",
    "\n",
    "            test_loss_single_fold.append(test_loss)\n",
    "            test_accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "            f1 = f1_score(all_labels, all_preds)\n",
    "            tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "            sensitivity = tp / (tp + fn)\n",
    "            specificity = tn / (tn + fp)\n",
    "            auc_score = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "            f1_list_single_fold.append(f1)\n",
    "            sensitivity_list_single_fold.append(sensitivity)\n",
    "            specificity_list_single_fold.append(specificity)\n",
    "            accuracy_list_single_fold.append(test_accuracy)\n",
    "            auc_score_list_single_fold.append(auc_score)\n",
    "            fpr, tpr, _ = roc_curve(all_labels, all_outputs)\n",
    "            fpr_list_single_fold.append(fpr)\n",
    "            tpr_list_single_fold.append(tpr)\n",
    "            epochs_single_fold.append(epoch)\n",
    "\n",
    "            print(f\"Test loss: {test_loss}, Accuracy: {test_accuracy:.2f}%\", f\"F1 Score: {f1:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}, AUC: {auc_score:.4f}\")\n",
    "            print(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "\n",
    "    max_f1 = max(f1_list_single_fold)\n",
    "    max_f1_index = f1_list_single_fold.index(max_f1)\n",
    "\n",
    "    f1_list_all_folds.append(f1_list_single_fold[max_f1_index])\n",
    "    f1_list_all_folds_train.append(f1_list_single_fold_train[max_f1_index])\n",
    "    sensitivity_list_all_folds.append(sensitivity_list_single_fold[max_f1_index])\n",
    "    sensitivity_list_all_folds_train.append(sensitivity_list_single_fold_train[max_f1_index])\n",
    "    specificity_list_all_folds.append(specificity_list_single_fold[max_f1_index])\n",
    "    specificity_list_all_folds_train.append(specificity_list_single_fold_train[max_f1_index])\n",
    "    accuracy_list_all_folds.append(accuracy_list_single_fold[max_f1_index])\n",
    "    accuracy_list_all_folds_train.append(accuracy_list_single_fold_train[max_f1_index])\n",
    "    auc_score_list_all_folds.append(auc_score_list_single_fold[max_f1_index])\n",
    "    auc_score_list_all_folds_train.append(auc_score_list_single_fold_train[max_f1_index])\n",
    "    fpr_list_all_folds.append(fpr_list_single_fold[max_f1_index])\n",
    "    tpr_list_all_folds.append(tpr_list_single_fold[max_f1_index])\n",
    "    test_loss_all_folds.append(test_loss_single_fold)\n",
    "    test_loss_max.append(test_loss_single_fold[max_f1_index])\n",
    "    train_loss_all_folds.append(train_loss_single_fold)\n",
    "    train_loss_max.append(train_loss_single_fold[max_f1_index])\n",
    "    epochs_all_fold.append(epochs_single_fold[max_f1_index])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy_list_all_folds}\")\n",
    "print(f\"Accuracy Train: {accuracy_list_all_folds_train}\")\n",
    "print(f\"F1 Score: {f1_list_all_folds}\")\n",
    "print(f\"F1 Score Train: {f1_list_all_folds_train}\")\n",
    "print(f\"Sensitivity: {sensitivity_list_all_folds}\")\n",
    "print(f\"Sensitivity Train: {sensitivity_list_all_folds_train}\")\n",
    "print(f\"Specificity: {specificity_list_all_folds}\")\n",
    "print(f\"Specificity Train: {specificity_list_all_folds_train}\")\n",
    "print(f\"AUC: {auc_score_list_all_folds}\")\n",
    "print(f\"AUC Train: {auc_score_list_all_folds_train}\")\n",
    "print(f\"Test Loss: {test_loss_max}\")\n",
    "print(f\"Train Loss: {train_loss_max}\")\n",
    "print(f\"Epochs: {epochs_all_fold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "executionInfo": {
     "elapsed": 1050,
     "status": "ok",
     "timestamp": 1770389659702,
     "user": {
      "displayName": "pietro cerveri",
      "userId": "02560383099036907904"
     },
     "user_tz": -60
    },
    "id": "V_nmuPLva2YE",
    "outputId": "3b961a3e-a987-423f-84c9-55cb5bafeb12"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Custom color palette\n",
    "colors = [\n",
    "    \"#E32947\",  # red\n",
    "    \"#F4A9B5\",  # pink\n",
    "    \"#155874\",  # dark blue\n",
    "    \"#29ABE2\",  # light blue\n",
    "    \"#E38D29\",  # orange\n",
    "    \"#E3DA29\",  # yellow\n",
    "    \"#7FE329\",  # green (flashy)\n",
    "    \"#BFBFBF\",  # grey\n",
    "    \"#00B050\",  # green (basic)\n",
    "    \"#7030A0\",  # purple\n",
    "    \"#996633\",  # brown\n",
    "]\n",
    "# Repeat colors if more folds than colors\n",
    "while len(colors) < len(fpr_list_all_folds):\n",
    "    colors += colors\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "tprs = []\n",
    "aucs = []\n",
    "\n",
    "for i in range(len(fpr_list_all_folds)):\n",
    "    interp_tpr = np.interp(mean_fpr, fpr_list_all_folds[i], tpr_list_all_folds[i])\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    auc_val = auc_score_list_all_folds[i] if i < len(auc_score_list_all_folds) else None\n",
    "    aucs.append(auc_val)\n",
    "    plt.plot(\n",
    "        fpr_list_all_folds[i],\n",
    "        tpr_list_all_folds[i],\n",
    "        color=colors[i],\n",
    "        label=f'Fold {i+1} (AUC={auc_val:.2f})'\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', label='Chance')\n",
    "\n",
    "# Mean and std curves\n",
    "tprs = np.array(tprs)\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "mean_auc = np.mean(aucs)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"#E32947\",\n",
    "    label=f'Mean ROC (AUC={mean_auc:.2f}±{std_auc:.2f})',\n",
    "    linewidth=2\n",
    ")\n",
    "plt.fill_between(\n",
    "    mean_fpr,\n",
    "    mean_tpr - std_tpr,\n",
    "    mean_tpr + std_tpr,\n",
    "    color=\"#E32947\",\n",
    "    alpha=0.2,\n",
    "    label='±1 std'\n",
    ")\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for all Folds')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 907,
     "status": "ok",
     "timestamp": 1770389672485,
     "user": {
      "displayName": "pietro cerveri",
      "userId": "02560383099036907904"
     },
     "user_tz": -60
    },
    "id": "Tlfy-3W0a2YE",
    "outputId": "c3becb53-beea-40e5-c5d2-2f995ce56bf2"
   },
   "outputs": [],
   "source": [
    "# Plot learning curves (train and test loss per epoch for each fold)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(len(train_loss_all_folds)):\n",
    "    plt.plot(train_loss_all_folds[i], color=colors[i % len(colors)], alpha=0.8, label=f'Train Fold {i+1}')\n",
    "    plt.plot(test_loss_all_folds[i], color=colors[i % len(colors)], alpha=0.8, linestyle='--', label=f'Test Fold {i+1}')\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curves (Train/Test Loss per Fold)\")\n",
    "plt.legend(ncol=2, fontsize=8)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1770389682519,
     "user": {
      "displayName": "pietro cerveri",
      "userId": "02560383099036907904"
     },
     "user_tz": -60
    },
    "id": "gy1W9uMra2YF",
    "outputId": "3a79e3c2-c189-4f50-decc-fdcb73817f67"
   },
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy_list_all_folds}\")\n",
    "print(f\"Accuracy Train: {accuracy_list_all_folds_train}\")\n",
    "print(f\"F1 Score: {f1_list_all_folds}\")\n",
    "print(f\"F1 Score Train: {f1_list_all_folds_train}\")\n",
    "print(f\"Sensitivity: {sensitivity_list_all_folds}\")\n",
    "print(f\"Sensitivity Train: {sensitivity_list_all_folds_train}\")\n",
    "print(f\"Specificity: {specificity_list_all_folds}\")\n",
    "print(f\"Specificity Train: {specificity_list_all_folds_train}\")\n",
    "print(f\"AUC: {auc_score_list_all_folds}\")\n",
    "print(f\"AUC Train: {auc_score_list_all_folds_train}\")\n",
    "print(f\"Test Loss: {test_loss_max}\")\n",
    "print(f\"Train Loss: {train_loss_max}\")\n",
    "print(f\"Epochs: {epochs_all_fold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1770389690674,
     "user": {
      "displayName": "pietro cerveri",
      "userId": "02560383099036907904"
     },
     "user_tz": -60
    },
    "id": "fWwJwvTKa2YF",
    "outputId": "73f03a68-6e6c-4e9c-b625-902f88ff74a6"
   },
   "outputs": [],
   "source": [
    "# Plot learning curves (train and test loss per epoch for each fold)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(train_loss_all_folds)):\n",
    "    plt.plot(train_loss_all_folds[i], color=colors[i % len(colors)], alpha=0.8, label=f'Train Fold {i+1}')\n",
    "    plt.plot(test_loss_all_folds[i], color=colors[i % len(colors)], alpha=0.8, linestyle='--', label=f'Test Fold {i+1}')\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curves (Train/Test Loss per Fold)\")\n",
    "plt.legend(ncol=2, fontsize=8)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1770389709186,
     "user": {
      "displayName": "pietro cerveri",
      "userId": "02560383099036907904"
     },
     "user_tz": -60
    },
    "id": "x8_2A--Ia2YF",
    "outputId": "aca6ae0b-4fc3-4964-a17e-6b6e0d474897"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Custom color palette\n",
    "colors = [\n",
    "    \"#E32947\",  # red\n",
    "    \"#F4A9B5\",  # pink\n",
    "    \"#155874\",  # dark blue\n",
    "    \"#29ABE2\",  # light blue\n",
    "    \"#E38D29\",  # orange\n",
    "    \"#E3DA29\",  # yellow\n",
    "    \"#7FE329\",  # green (flashy)\n",
    "    \"#BFBFBF\",  # grey\n",
    "    \"#00B050\",  # green (basic)\n",
    "    \"#7030A0\",  # purple\n",
    "    \"#996633\",  # brown\n",
    "]\n",
    "# Repeat colors if more folds than colors\n",
    "while len(colors) < len(fpr_list_all_folds):\n",
    "    colors += colors\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "tprs = []\n",
    "aucs = []\n",
    "\n",
    "for i in range(len(fpr_list_all_folds)):\n",
    "    interp_tpr = np.interp(mean_fpr, fpr_list_all_folds[i], tpr_list_all_folds[i])\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    auc_val = auc_score_list_all_folds[i] if i < len(auc_score_list_all_folds) else None\n",
    "    aucs.append(auc_val)\n",
    "    plt.plot(\n",
    "        fpr_list_all_folds[i],\n",
    "        tpr_list_all_folds[i],\n",
    "        color=colors[i],\n",
    "        label=f'Fold {i+1} (AUC={auc_val:.2f})'\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='black', linestyle='--', label='Chance')\n",
    "\n",
    "# Mean and std curves\n",
    "tprs = np.array(tprs)\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "mean_auc = np.mean(aucs)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"#E32947\",\n",
    "    label=f'Mean ROC (AUC={mean_auc:.2f}±{std_auc:.2f})',\n",
    "    linewidth=2\n",
    ")\n",
    "plt.fill_between(\n",
    "    mean_fpr,\n",
    "    mean_tpr - std_tpr,\n",
    "    mean_tpr + std_tpr,\n",
    "    color=\"#E32947\",\n",
    "    alpha=0.2,\n",
    "    label='±1 std'\n",
    ")\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for all Folds')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
